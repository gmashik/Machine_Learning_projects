# -*- coding: utf-8 -*-
"""Spam_Detection_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12qc5OwvqOjnYW-xtkYoVVTSCB3Bujv7g

# **Text Message Spam Detection Project**

<table align="center">
  
  <td align="center"><a target="_blank" href="https://colab.research.google.com/drive/12qc5OwvqOjnYW-xtkYoVVTSCB3Bujv7g?authuser=2#scrollTo=9JZLbUZK8UTt">
        <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTR7EtP27gljpJg91k2DVoRgkB84hkMl78bOA&usqp=CAU""  style="padding-bottom:5px;" />
        
  Run this project in Google Colab</a></td>
  
</table>

We'll be using a dataset from the [UCI datasets](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)! The file we are using contains a collection of more than 5 thousand SMS phone messages.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import string
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB

"""load **NLTK** stopwords from the download shell"""

#after running nltk.download_shell() press l to see packages 
#we need stopwords pacakage for this project so press
# d for download and type stopwords in the shell and press enter

nltk.download_shell()

"""Load our data"""

!wget https://raw.githubusercontent.com/gmashik/Machine_learning_projects/master/Spam_Filtering_project/data/SMSSpamCollection1

#before load please mount your google drive 
msgdata=pd.read_csv('SMSSpamCollection1',sep='\t',names=['label','message'])

msgdata.head(5)

"""Using these labeled ham and spam examples, we'll **train a machine learning model to learn to discriminate between ham/spam automatically**. Then, with a trained model, we'll be able to **classify arbitrary unlabeled messages** as ham or spam. We'll explore our data first"""

msgdata.groupby('label').describe()

"""We can see we have 4516 unique ham/not spam message and most frequent is "Sorry, I'll call later". We have got this 30 times. Similarly we have 653 unique spam email. and highest frequency of the repeated message is 4.

As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of feature engineering. We need a good domain knowledge on the data. Feature engineering is a very large part of spam detection in general. 
We'll make a new column to detect how long the text messages are:
"""

msgdata['length']=msgdata['message'].apply(len)

msgdata.head()

sns.distplot(msgdata['length'],bins=30,kde=False)

sns.distplot(msgdata['length'],bins=200,kde=False)

"""We've got a bimodal distriubution with increased number of bins"""

msgdata['length'].hist(bins=150)

"""If we look carefully we can see that some outliers. Let's try to explain why the x-axis goes all the way to 1000ish, this must mean that there is some really long message!"""

msgdata.length.describe()

"""So we have message of length 910.

The longest message is:
"""

msgdata[msgdata['length']==910]['message'].iloc[0]

msgdata.hist(column='length',by='label',bins=40,figsize=(13,5))

"""Very interesting! Through just basic EDA we've been able to discover a trend that spam messages (their length) tend to have more characters.

Our main issue with our data is that it is all in text format (strings). The classification algorithms that we've learned about so far will need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the the bag-of-words approach, where each unique word in a text will be represented by one number.

In this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).

As a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here.

Let's create a function that will process the string in the message column, then we can just use apply() in pandas do process all the text in the DataFrame.
"""

def text_processing(instr): #tokenizing 
  """
  1. Remove punctuation form each message 
  2. Remove Stop words
  3. Return processed words
  """
  puncremove=[word for word in instr if word not in string.punctuation]
  puncremove=''.join(puncremove)
  return [word for word in puncremove.split() if word.lower() not in stopwords.words('english')]

X_train, X_test, y_train, y_test = train_test_split(msgdata['message'], msgdata['label'], test_size=0.3, random_state=101)

print(len(X_train), len(X_test), len(X_train) + len(X_test))

"""We've put 70% data as training set and 30% data as test data

**We'll create scikit-learn pipeline**

Pipeline for naive bayes classifies with Multinomial naive Bayes
"""

pipeline1 = Pipeline([
    ('Bag_Of_Words', CountVectorizer(analyzer=text_processing)),  # strings to token integer counts
    ('TF-IDF', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('Classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])

"""Pipeline for random forest classifier"""

pipeline2 = Pipeline([
    ('Bag_Of_Words', CountVectorizer(analyzer=text_processing)),  # strings to token integer counts
    ('TF-IDF', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('Classifier', RandomForestClassifier()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])

"""pipeline for Bernoulli Naive Bayes classifier"""

pipeline3 = Pipeline([
    ('Bag_Of_Words', CountVectorizer(analyzer=text_processing)),  # strings to token integer counts
    ('TF-IDF', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('Classifier',BernoulliNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])

pipeline1.fit(X_train,y_train)

pipeline2.fit(X_train,y_train)

pipeline3.fit(X_train,y_train)

p_test1=pipeline1.predict(X_test)

p_test2=pipeline2.predict(X_test)

p_test3=pipeline3.predict(X_test)

"""# **Classification Report for Naive Bayes Classifier**"""

print(classification_report(p_test1,y_test))

"""# **Classification Report for Random Forest Classifier**"""

print(classification_report(p_test2,y_test))

"""## **Classification Report for Bernoulli Naive Bayes Classifier**"""

print(classification_report(p_test3,y_test))

#@title Naive Bayes classifier spam detector { run: "auto", vertical-output: true, display-mode: "both" }
message = "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL" #@param {type:"string"}
x=[]
x.append(message)
if len(x[0])==0:
  print("Please enter your message")
elif pipeline1.predict(x)[0]=='spam':
  print("The message you have given is a spam")
else:
  print("The message you have given is not spam")
x.clear()

#@title Random forest classifier spam detector { run: "auto", vertical-output: true, display-mode: "both" }
message = "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL" #@param {type:"string"}
x=[]
x.append(message)
if len(x[0])==0:
  print("Please enter your message")
elif pipeline2.predict(x)[0]=='spam':
  print("The message you have given is a spam")
else:
  print("The message you have given is not spam")
x.clear()

#@title Bernoulli Naive Bayes classifier spam detector { run: "auto", vertical-output: true, display-mode: "both" }
message = "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL" #@param {type:"string"}
x=[]
x.append(message)
if len(x[0])==0:
  print("Please enter your message")
elif pipeline3.predict(x)[0]=='spam':
  print("The message you have given is a spam")
else:
  print("The message you have given is not spam")
x.clear()

"""## **Conclusion**

We can see the random forest classifier performance is well than others!!!
"""

