# -*- coding: utf-8 -*-
"""Housing_price_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlZBr467WWJdQniOjxWASoHP3UVzLYOq

# **Housing Price modeling project**

<table align="center">
  
  <td align="center"><a target="_blank" href="https://colab.research.google.com/drive/1_zqgsE4M3M7xjlDgkoPYiRNBXg4OT4rV?authuser=2#scrollTo=spJCE-HI8CNk">
        <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTR7EtP27gljpJg91k2DVoRgkB84hkMl78bOA&usqp=CAU""  style="padding-bottom:5px;" />
        
  Run this project in Google Colab</a></td>
  
</table>

Robin is a real estate agent and wants some help predicting housing prices for regions in the USA. It would be great if we could somehow create a model for him that allows her to put in a few features of a house and returns back an estimate of what the house would sell for.


He provided some information about a bunch of houses in regions of the United States,it is all in the data set: USA_Housing.csv.

The data contains the following columns:

* 'Avg. Area Income': Avg. Income of residents of the city house is located in.
* 'Avg. Area House Age': Avg Age of Houses in same city
* 'Avg. Area Number of Rooms': Avg Number of Rooms for Houses in same city
* 'Avg. Area Number of Bedrooms': Avg Number of Bedrooms for Houses in same city
* 'Area Population': Population of city house is located in
* 'Price': Price that the house sold at
* 'Address': Address for the house
"""

from google.colab import files
files.upload()

import numpy as np
import pandas as pd

hdat=pd.read_csv("USA_Housing.csv")

hdat.head()

hdat.info()

hdat.describe()

col=hdat.columns

len(col)

X=hdat[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
       'Avg. Area Number of Bedrooms', 'Area Population']]
y=hdat['Price']

from sklearn.preprocessing import normalize

X=sklearn.preprocessing.normalize(X, norm='l2')
X=pd.DataFrame(X,columns=col[0:5])
X.head()

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

lrmodel=LinearRegression()

lrmodel.fit(X_train,y_train)

p_test=lrmodel.predict(X_test)

from sklearn import metrics



print('Mean absolute error:', metrics.mean_absolute_error(y_test, p_test))
print('Mear Squared error:', metrics.mean_squared_error(y_test, p_test))
print('Root Mear Squared error:', np.sqrt(metrics.mean_squared_error(y_test,p_test)))

import seaborn as sns
sns.distplot((y_test-p_test),bins=50)

import matplotlib.pyplot as plt
plt.scatter(y_test,p_test)

print(lrmodel.coef_)

import tensorflow as tf
from tensorflow.keras.layers import Dense,Input,Flatten,Dropout
from tensorflow.keras.models import Model

tf.__version__

X=hdat[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
       'Avg. Area Number of Bedrooms', 'Area Population']]
y=hdat['Price']

X.head()

y.head()

hdat.head()

train_dataset = hdat.sample(frac=0.8,random_state=0)
test_dataset = hdat.drop(train_dataset.index)

test_dataset.head()

train_dataset.head()

train_labels = train_dataset.pop('Address')
test_labels = test_dataset.pop('Address')

y_train=train_dataset['Price']
y_test=test_dataset['Price']

y_train.head()

train_labels = train_dataset.pop('Price')
test_labels = test_dataset.pop('Price')

train_dataset.head()

test_dataset.head()



train_dataset.keys()

len(test_dataset.keys())

train_stats = train_dataset.describe()
train_stats = train_stats.transpose()

train_stats

def norm(x):
  return (x - train_stats['mean']) / train_stats['max']
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)

normed_train_data.head()

#build Model
i=Input(shape=len(train_dataset.keys()))
x=Dense(512,activation='relu')(i)
x=Dense(512,activation='relu')(x)
x=Dense(512,activation='relu')(x)
x=Dense(1)(x)

model=Model(i,x)

model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),loss='mse',metrics=['mae','mse'])

r=model.fit(normed_train_data,y_train,validation_split=0.2,epochs=100,callbacks=[tfdocs.modeling.EpochDots()])

!pip install git+https://github.com/tensorflow/docs

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

hist = pd.DataFrame(r.history)
hist['epoch'] = r.epoch
hist.tail()

plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)

import matplotlib.pyplot as plt
plotter.plot({'Basic': r}, metric = "mae")

plt.ylabel('MAE [Price]')

plotter.plot({'Basic': r}, metric = "mse")

plt.ylabel('MSE [price]')

loss, mae, mse = model.evaluate(normed_test_data, y_test, verbose=2)

print("Testing set Mean Abs Error: {:5.2f}".format(mae))

test_predictions = model.predict(normed_test_data).flatten()
a = plt.axes(aspect='equal')
plt.scatter(y_test, test_predictions)
plt.xlabel('True price ')
plt.ylabel('Price predictions ')

error = test_predictions - y_test
plt.hist(error, bins = 250)
plt.xlabel("Prediction Error ")
_ = plt.ylabel("Count")

import seaborn as sns
sns.distplot((y_test-test_predictions),bins=250);

sns.heatmap(hdat.corr())

sns.pairplot(hdat)

